---
title: SVM(支持向量机)
layout: post
share: false
---

# 1.预备知识

### 1.1 $KKT$ 条件

#### 无约束优化
对于变量 $x\in\mathbb{R}^n$ 的函数 $f(x)$ ,无约束优化问题如下:

$$\min_xf(x)$$

直接找到使目标函数导数为 $0$ 的点即可，即 $f'(x)=0$ ,如果没有解析解可以使用梯度下降或牛顿法等通过迭代使 $x$ 沿负梯度方向逐渐逼近最小值点。

##### 等式约束

如下等式约束问题：

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~h_i(x) = 0 , i = 1,2,...,m \\ 
\end{aligned}$$

约束条件会将解的范围限定在一个可行区域，此时不一定能找到 $f'(x)$ 为 $0$ 的点，只需找到可行区域内使得 $f(x)$ 最小的点即可，一般使用拉格朗日乘子法来进行求解，引入拉格朗日乘子 $\alpha\in\mathbb{R}^m$ ,构建拉格朗日函数：

$$L(x,\alpha)=f(x)+\sum_{i=1}^m\alpha_ih_i(x)$$

并分别对 $\alpha$ 和 $x$ 求偏导：

$$\left 
\{ 
\begin{aligned}  
\frac{\partial L(x,\alpha)}{\partial x}= 0  \\ 
\frac{\partial L(x,\alpha)}{\partial \alpha}= 0 
\end{aligned} 
\right.$$

求得 $x$ 、 $\alpha$ 的值，将 $x$ 代入 $f(x)$ 即为在约束条件 $h_i(x)$ 下的可行解。下面用一个示例来进行说明，对于二维的目标函数 $f(x,y)$ ,在平面中画出   $f(x,y)$ 的等高线，如下图虚线，并给出一个约束等式 $h(x,y)=0$ ,如下图绿线，目标函数 $f(x,y)$ 与约束 $g(x,y)$ 只可能相交，相切或没交集，只有相交或相切时才有可能是解，而且只有相切才可能得到可行解，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小。

![](https://darknessbeforedawn.github.io/test-book/images/SVM1.png)

因此,拉格朗日乘子法取得极值的必要条件是目标函数与约束函数相切，这时两者的法向量是平行的，即

$$f'(x)-\alpha h'(x)=0$$

所以只要满足上述等式，且满足约束 $h_i(x) = 0 , i = 1,2,…,m$ 即可得到解，联立起来，正好得到就是拉格朗日乘子法。以上为拉格朗日乘子法的几何推导。

##### 不等式约束

给定如下不等式约束问题：

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~g(x) \leq 0 , i = 1,2,...,m \\ 
\end{aligned}$$

对应的拉格朗日函数：

$$
L(x,\lambda) = f(x)+\lambda g(x)
$$

这时的可行解必须落在约束区域 $g(x)$ 之内，稀土给出了目标函数的等高线与约束：

![](https://darknessbeforedawn.github.io/test-book/images/SVM2.png)

由图可知可行解 $x$ 只能在 $g(x) \leq 0$的区域里:

(1)当可行解 $x$ 落在 $g(x)<0$ 的区域内，此时直接极小化 $f(x)$ 即可；

(2)当可行解 $x$ 落在 $g(x)=0$ 即边界上，此时等价于等式约束问题。

当约束区域包含目标函数原有的可行解时，此时加上约束可行解仍然落在约束区域内部，对应 $g(x)<0$ 的情况，这时约束条件不起作用；当约束区域不包含目标函数原有的可行解时，此时加上约束后可行解落在边界 $g(x)=0$ 上。下图分别描述了两种情况，右图表示加上约束可行解会落在约束区域的边界上。

![](https://darknessbeforedawn.github.io/test-book/images/SVM3.png)

以上两种情况，要么可行解落在约束边界上即得 $g(x)=0$ ，要么可行解落在约束区域内部，此时约束不起作用，令 $\lambda = 0$ 消去约束即可，所以无论哪种情况都会得到：

$$\lambda g(x)=0$$

在等式约束优化中，约束函数与目标函数的梯度只要满足平行即可，而在不等式约束中则不然，若 $\lambda \neq 0$，则可行解 $x$ 是落在约束区域的边界上，这时可行解应尽量靠近无约束时的解，所以在约束边界上，目标函数的负梯度方向应该远离约束区域朝向无约束时的解，此时正好可得约束函数的梯度方向与目标函数的负梯度方向应相同：

$$-\nabla_xf(x)=\lambda\nabla_xg(x)$$

上式需要满足 $\lambda > 0$ ，这个问题可以举一个形象的例子，假设你去爬山，目标是山顶，但有一个障碍挡住了通向山顶的路，所以只能沿着障碍爬到尽可能靠近山顶的位置，然后望着山顶叹叹气，这里山顶便是目标函数的可行解，障碍便是约束函数的边界，此时的梯度方向一定是指向山顶的，与障碍的梯度同向，下图描述了这种情况:

![](https://darknessbeforedawn.github.io/test-book/images/SVM4.png)

对于不等式约束，只要满足一定的条件，依然可以使用拉格朗日乘子法解决，这里的条件便是 $KKT$ 条件。

对于以下约束问题:

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~h_i(x)=0,i = 1,2,...,m \\
&~~~~~~~~~~g(x) \leq 0 , j = 1,2,...,n \\ 
\end{aligned}$$

对应拉格朗日函数：

$$L(x,\alpha,\beta)=f(x)+\sum_{i=1}^m\alpha_ih_i(x)+\sum_{j=1}^n\beta_ig_i(x)$$

则不等式约束后的可行解 $x$ 需要满足的 $KKT$ 条件为：

$$\begin{align}
\nabla_x L(x,\alpha,\beta) &= 0   \\ 
\beta_jg_j(x) &= 0  , \ j=1,2,...,n\\ 
h_i(x)&= 0 , \ i=1,2,...,m  \\ 
g_j(x) &\le 0  , \  j=1,2,...,n  \\ 
\beta_j &\ge  0 , \ j=1,2,...,n  \\ 
\end{align} $$

满足 $KKT$ 条件后极小化拉格朗日函数即可得到在不等式约束条件下的可行解。 $KKT$ 条件:

(1) ：拉格朗日取得可行解的必要条件；

(2) ：这就是以上分析的一个比较有意思的约束，称作松弛互补条件；

(3)(4) ：初始的约束条件；

(5) ：不等式约束的 Lagrange Multiplier 需满足的条件。

主要的 $KKT$ 条件便是 (3) 和 (5) ，只要满足这俩个条件便可直接用拉格朗日乘子法， $SVM$ 中的支持向量便是来自于此，需要注意的是 KKT 条件与对偶问题也有很大的联系。

### 1.2 对偶问题

对于任意一个带约束的优化都可以写成如下形式：

$$\begin{aligned}  
&\min_x \  f(x)  \\  
&s.t.  \ \ \ h_i(x) = 0 , \  i = 1,2,...,m \ \\  
& \ \ \ \ \ \ \ \ \ \   g_j(x) \le 0, \  j = 1,2,...,n 
\end{aligned}$$

如果 $g_j(x)$ 全是凸函数，并且 $h_i(x)$ 全是仿射函数（ $Ax+b$ 的形式），上述优化就是一个凸优化问题，凸优化极值唯一。

定义如下 $Lagrangian$ ：

$$
L(x,\alpha,\beta) =f(x) + \sum_{i=1}^m \alpha_i h_i(x) + \sum_{j=1}^n\beta_jg_j(x)
$$

它通过一些系数把约束条件和目标函数结合起来，将带约束的优化问题转化为无约束问题。

现在我们针对参数 $\alpha,\beta$ 对 $L(x,\alpha,\beta)$ 取最大值，令:

$$
Z(x) =\max_{\alpha_i,\beta_j \geq 0}L(x,\alpha,\beta)
$$

满足约束条件的 $x$ 使得 $h_i(x)=0$ ,并且 $g_j(x) \leq 0,\beta_j \geq 0$ ,则有 $\beta_jg_j(x) \leq 0$ .因此对于满足约束条件的 $x$ 有 $f(x)=Z(x)$ .而对于那些不满足约束条件的 $x$ 有 $Z(x)=\infty$ ,这样将导致问题无解，因此必须满足约束条件。这样一来，原始带约束的优化问题等价于如下无约束优化问题：

$$
\min_x f(x) = \min_xZ(x) = \min_x \max_{\alpha_i,\beta_j \geq 0}L(x,\alpha,\beta)
$$

这个问题称作原问题(primal problem),与之相对应的为对偶问题(dual problem),器形式非常类似，只是把 $\min,\max$ 交换了一下：

$$
\max_{\alpha_i,\beta_j \geq 0}\min_xL(x,\alpha,\beta)
$$

原问题是在最大值中取最下的那个，对偶问题是在最小值取最大的那个。令：

$$
D(\alpha, \beta)= \min_xL(x,\alpha,\beta)
$$

如果原问题的最小值记为 $p^*$ ,那么对于所有 $\alpha_i,\beta_j \geq 0$ 有：

$$
D(\alpha, \beta) \leq p^*
$$

由于对于极值点(包括所有满足约束条件的点) $x^*$ , 并且 $\beta_j \geq 0$ ,总有 :

$$
\sum_{i=1}^m \alpha_i h_i(x^*) + \sum_{j=1}^n\beta_jg_j(x^*) \leq 0
$$ 

因此

$$
L(x^*,\alpha,\beta) = f(x^*) +\sum_{i=1}^m \alpha_i h_i(x^*) + \sum_{j=1}^n\beta_jg_j(x^*) \leq f(x^*)
$$

于是

$$
D(\alpha, \beta)= \min_xL(x,\alpha,\beta) \leq L(x^*,\alpha,\beta) \leq f(x^*) = p^*
$$

因此 

$$
\max_{\alpha_i,\beta_j \geq 0}D(\alpha, \beta)
$$

实际上就是原问题的最大下界，最大下界离我们要逼近的值最近。记对偶问题的最优值为 $d^*$ ，则有：

$$d^* \leq p^*$$

这个性质叫做弱对偶性（weak duality），对于所有优化问题都成立，即使原始问题非凸。这里还有两个概念： $f(x)–D(\alpha, \beta)$ 叫做对偶间隔（duality gap）， $p^*–d^*$ 叫做最优对偶间隔（optimal duality gap）。无论原始问题是什么形式，对偶问题总是一个凸优化的问题，这样对于那些难以求解的原始问题 （甚至是 NP 问题），均可以通过转化为偶问题，通过优化这个对偶问题来得到原始问题的一个下界， 与弱对偶性相对应的有一个强对偶性（strong duality） ，强对偶即满足：

$$d^* = p^*$$

强对偶是一个非常好的性质，因为在强对偶成立的情况下，可以通过求解对偶问题来得到原始问题的解，在 $SVM$ 中就是这样做的。当然并不是所有的对偶问题都满足强对偶性 ，在 $SVM$ 中是直接假定了强对偶性的成立，其实只要满足一些条件，强对偶性是成立的，比如说 $Slater$ 条件与 $KKT$ 条件。

#### $Slater$ 条件

若原始问题为凸优化问题，且存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $g_j(x)\leq 0$ 中的“ $≤$ ”严格取到“ $<$ ”，即存在 $x$ 满足 $g_j(x)<0 ,i=1,2,…,n$ ，则存在 $x^*,α^*,β^*$ 使得 $x^*$ 是原始问题的解， $α^*,β^*$ 是对偶问题的解，且满足：

$$p^* = d^* = L(x^*,\alpha^* ,\beta^*)$$

也就是说如果原始问题是凸优化问题并且满足 $Slater$ 条件的话，那么强对偶性成立。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。例如，对于某些非凸优化的问题，强对偶也成立。$SVM$ 中的原始问题 是一个凸优化问题（二次规划也属于凸优化问题），$Slater$ 条件在 $SVM$ 中指的是存在一个超平面可将数据分隔开，即数据是线性可分的。当数据不可分时，强对偶是不成立的，这个时候寻找分隔平面这个问题本身也就是没有意义了，所以对于不可分的情况预先加个 $kernel$ 就可以了。

#### $KKT$ 条件

假设 $x^*$ 与 $α^*,β^*$ 分别是原始问题（并不一定是凸的）和对偶问题的最优解，且满足强对偶性，则相应的极值的关系满足：

$$
\begin{aligned}  f(x^*) &= d^* = p^* =D(\alpha^*,\beta^*)  \\  &=\min_x f(x)+ \sum_{i = 1}^m \alpha_i^*h_i(x) + \sum_{j=1}^n\beta_j^*g_j(x) \\  & \le f(x^*)+ \sum_{i = 1}^m \alpha_i^*h_i(x^*) + \sum_{j=1}^n\beta_j^*g_j(x^*) \\ &\le f(x^*)  \end{aligned}
$$

由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^*$ 是 $L(x,\alpha^*,\beta^*)$ 的一个极值点，因此 $L(x,\alpha^*,\beta^*)$ 在 $x^*$ 处的梯度为0，即: 

$$\nabla_{x^*} L(x,\alpha^*,\beta^*) = 0$$

由第二个不等式，并且 $\beta_j^*g_j(x^*)$ 都是非正的，因此有：

$$\beta_j^*g_j(x^*)=0, j=1,\cdots,m$$

显然，如果 $\beta_j^* > 0$ ,那么必定有 $g_j(x^*)=0$ ,如果 $g_j(x^*) <0$ ,那么可以得到 $\beta_j^* = 0$ 。再将其他一些显而易见的条件写到一起，就是传说中的 KKT (Karush-Kuhn-Tucker) 条件：

$$
\begin{align}
\nabla_x L(x,\alpha,\beta) &= 0   \\ 
\beta_jg_j(x) &= 0  , \ j=1,2,...,n\\ 
h_i(x)&= 0 , \ i=1,2,...,m  \\ 
g_j(x) &\le 0  , \  j=1,2,...,n  \\ 
\beta_j &\ge  0 , \ j=1,2,...,n  \\ 
\end{align}
$$

总结来说就是说任何满足强对偶性的优化问题，只要其目标函数与约束函数可微，任一对原始问题与对偶问题的解都是满足 $KKT$ 条件的。即满足强对偶性的优化问题中，若 $x^*$ 为原始问题的最优解，$α^*,β^*$ 为对偶问题的最优解，则可得 $x^*,α^*,β^*$ 满足 $KKT$ 条件。

上面只是说明了必要性，当满足原始问题为凸优化问题时，必要性也是满足的，也就是说当原始问题是凸优化问题,且存在 $x^*,α^*,β^*$ 满足 $KKT$ 条件，那么它们分别是原始问题和对偶问题的极值点并且强对偶性成立.

##### 证明
首先原始问题是凸优化问题，固定 $α^*,β^*$ 之后对偶问题 $D(α^*,β^*)$ 也是一个凸优化问题，$x^*$ 是 $L(x,α^*,β^*)$ 的极值点：

$$
\begin{aligned}  
D(\alpha^*,\beta^*)  &= \min_x L(x,\alpha^*,\beta^*) \\ 
&= L(x^*,\alpha^*,\beta^*) \\ 
& =f(x^*)+\sum_{i=1}^m\alpha_i^*h_i(x^*)+\sum_{j=1}^n\beta_j^*g_j(x^*) \\  
&= f(x^*)  
\end{aligned}
$$

最后一个式子是根据 $KKT$ 条件中的 $h_i(x)=0$ 与 $\beta_jg_j(x)=0$ 得到的。这样一来，就证明了对偶间隔为零，也就是说，强对偶成立。


对于一个约束优化问题，找到其对偶问题，当弱对偶成立时，可以得到原始问题的一个下界。而如果强对偶成立，则可以直接求解对偶问题来解决原始问题。 $SVM$ 就是这样的。对偶问题由于性质良好一般比原始问题更容易求解，在 $SVM$ 中通过引入对偶问题可以将问题表示成数据的内积形式从而使得 kernel trick 的应用更加自然。此外，还有一些情况会同时求解对偶问题与原始问题 ，比如在迭代求解的过程中，通过判断对偶间隔的大小，可以得出一个有效的迭代停止条件。

# 2.支持向量机

## 2.1 线性可分支持向量机与硬间隔最大化

### 2.1.1 线性可分支持向量机

SVM 一直被认为是效果最好的现成可用的分类算法之一，$SVM$ 内容比较多，学习 $SVM$ 首先要从线性分类器开始。考虑一个二分类问题，给定训练数据集

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},~~x_i\in\mathcal{X}=\mathbf{R}^n,~~y_i\in\mathcal{Y}=\{+1,-1\},~~i=1,2,\cdots,n$$

其中 $x_i$ 为第 $i$ 个特征向量， $y_i$ 为 $x_i$ 的类标记（有些地方会选 0 和 1 ，当然其实分类问题选什么都无所谓，只要是两个不同的数字即可，不过这里选择 +1 和 -1 是为了方便 SVM 的推导），学习的目标是在特征空间中找到一个分离超平面(线性分类器),其方程可表示为：

$$w^Tx+b=0$$

其中 $w=(w_1;w_2;\cdots;w_d)$ 为法向量，决定了超平面的方向； $b$ 为位移项，决定了超平面与原点之间的距离。在二维空间中的例子就是一条直线。通过这个超平面可以把两类数据分隔开来，一部分是正类，一部分是负类，法向量指向的一侧为正类，另一侧为负类。这里我们首先来说明线性可分的情况，对于线性不可分的情况后边会进行分析。

### 2.1.2 函数间隔与几何间隔

![](https://darknessbeforedawn.github.io/test-book/images/SVM5.png)
 
如图所示，两种标记的点分别代表两个类别，直线表示一个可行的超平面。将数据点 $x$ 代入 $f(x)$ 中，如果得到的结果小于 0 ，则赋予其类别 -1 ，如果大于 0 则赋予类别 1 。对于 $f(x)$ 的绝对值很小(包括)的情况，是很难处理的，因为细微的变动（比如超平面稍微转一个小角度）就有可能导致结果类别的改变，也就是越接近超平面的点越“难”分隔。

在超平面 $w^Tx+b=0$ 确定的情况下 $$|w^Tx+b|$$ 能够相对的表示点 $x$ 与超平面的距离。 $w^Tx+b$ 的符号与类标记 $y$ 的符号是否一致可判断分类是否正确,因此 $y(w^Tx+b)$ 可以用来表示分类的正确性及确信度。

隔函数间隔（functional margin）：

$$\hat\gamma_i = y_i(w^Tx_i+b)=y_if(x_i)$$

而超平面关于 $T$ 中所有样本点 $(x_i,y_i)$ 的函数间隔最小值（ $i$ 表示第 $i$ 个样本），便为超平面关于训练数据集 $T$ 的函数间隔：

$$\hat\gamma=\min_{i=1,2,\cdots,n} \hat\gamma_i $$

这样定义的函数间隔有问题，如果成比例的改变 $w$ 和 $b$（如将它们改成 $2w$ 和 $2b$ ），则函数间隔的值 $f(x)$ 却变成了原来的2倍（虽然此时超平面没有改变）。但可以通过对超平面的法向量 $w$ 加一些约束，如规范化 $\|w\|=1$ ,使间隔确定。这时的函数间隔即为几何间隔(geometrical margin)。

![](https://darknessbeforedawn.github.io/test-book/images/SVM6.png)

如图所示，对于一个点 $x$ ，令其垂直投影到超平面上的对应的为 $x_0$ ，由于 $w$ 是垂直于超平面的一个向量， $\gamma$ 为样本 $x$ 到超平面的距离,则有：

$$x=x_0+\gamma \frac{w}{\|w\|}$$

其中 $\|w\|$ 为 $w$ 的二阶范数（范数是一个类似于模的表示长度的概念）， $\frac{w}{\|w\|}$ 是单位向量（一个向量除以它的模称之为单位向量）。又由于 $x_0$ 是超平面上的点，满足 $f(x0)=0$ ，代入超平面的方程 $w^Tx+b=0$ ，可得 $w^Tx_0=-b$。

$x=x_0+\gamma \frac{w}{\|w\|}$ 两边同时乘以 $w^T$ , 再有 $w^Tw = \|w\|^2$ ,可得：

$$\gamma = \frac{w^T+b}{\|w\|} = \frac{f(x)}{\|w\|}$$

为了得到 $\gamma$ 的绝对值，令 $\gamma$ 乘上对应的类别 $y$，即可得出几何间隔（用 $\tilde{\gamma}$ 表示）的定义：


$$\tilde{\gamma} = y\gamma =\frac{\hat\gamma}{\|w\|}$$

几何间隔就是函数间隔除以 $\|w\|$ ，而且函数间隔 $y(w^Tx+b) = yf(x)$ 实际上就是$$|f(x)|$$，只是人为定义的一个间隔度量，而几何间隔$$\frac{|f(x)|}{\|w\|}$$才是直观上的点到超平面的距离。

### 2.1.2 间隔最大化