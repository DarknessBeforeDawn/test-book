---
title: SVM(支持向量机)
layout: post
share: false
---

# 1.预备知识

### 1.1 $KKT$ 条件

#### 无约束优化
对于变量 $x\in\mathbb{R}^n$ 的函数 $f(x)$ ,无约束优化问题如下:

$$\min_xf(x)$$

直接找到使目标函数导数为 $0$ 的点即可，即 $f'(x)=0$ ,如果没有解析解可以使用梯度下降或牛顿法等通过迭代使 $x$ 沿负梯度方向逐渐逼近最小值点。

##### 等式约束

如下等式约束问题：

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~h_i(x) = 0 , i = 1,2,...,m \\ 
\end{aligned}$$

约束条件会将解的范围限定在一个可行区域，此时不一定能找到 $f'(x)$ 为 $0$ 的点，只需找到可行区域内使得 $f(x)$ 最小的点即可，一般使用拉格朗日乘子法来进行求解，引入拉格朗日乘子 $\alpha\in\mathbb{R}^m$ ,构建拉格朗日函数：

$$L(x,\alpha)=f(x)+\sum_{i=1}^m\alpha_ih_i(x)$$

并分别对 $\alpha$ 和 $x$ 求偏导：

$$\left 
\{ 
\begin{aligned}  
\frac{\partial L(x,\alpha)}{\partial x}= 0  \\ 
\frac{\partial L(x,\alpha)}{\partial \alpha}= 0 
\end{aligned} 
\right.$$

求得 $x$ 、 $\alpha$ 的值，将 $x$ 代入 $f(x)$ 即为在约束条件 $h_i(x)$ 下的可行解。下面用一个示例来进行说明，对于二维的目标函数 $f(x,y)$ ,在平面中画出   $f(x,y)$ 的等高线，如下图虚线，并给出一个约束等式 $h(x,y)=0$ ,如下图绿线，目标函数 $f(x,y)$ 与约束 $g(x,y)$ 只可能相交，相切或没交集，只有相交或相切时才有可能是解，而且只有相切才可能得到可行解，因为相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小。

![](https://darknessbeforedawn.github.io/test-book/images/SVM1.png)

因此,拉格朗日乘子法取得极值的必要条件是目标函数与约束函数相切，这时两者的法向量是平行的，即

$$f'(x)-\alpha h'(x)=0$$

所以只要满足上述等式，且满足约束 $h_i(x) = 0 , i = 1,2,…,m$ 即可得到解，联立起来，正好得到就是拉格朗日乘子法。以上为拉格朗日乘子法的几何推导。

##### 不等式约束

给定如下不等式约束问题：

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~g(x) \leq 0 , i = 1,2,...,m \\ 
\end{aligned}$$

对应的拉格朗日函数：

$$
L(x,\lambda) = f(x)+\lambda g(x)
$$

这时的可行解必须落在约束区域 $g(x)$ 之内，稀土给出了目标函数的等高线与约束：

![](https://darknessbeforedawn.github.io/test-book/images/SVM2.png)

由图可知可行解 $x$ 只能在 $g(x) \leq 0$的区域里:

(1)当可行解 $x$ 落在 $g(x)<0$ 的区域内，此时直接极小化 $f(x)$ 即可；

(2)当可行解 $x$ 落在 $g(x)=0$ 即边界上，此时等价于等式约束问题。

当约束区域包含目标函数原有的可行解时，此时加上约束可行解仍然落在约束区域内部，对应 $g(x)<0$ 的情况，这时约束条件不起作用；当约束区域不包含目标函数原有的可行解时，此时加上约束后可行解落在边界 $g(x)=0$ 上。下图分别描述了两种情况，右图表示加上约束可行解会落在约束区域的边界上。

![](https://darknessbeforedawn.github.io/test-book/images/SVM3.png)

以上两种情况，要么可行解落在约束边界上即得 $g(x)=0$ ，要么可行解落在约束区域内部，此时约束不起作用，令 $\lambda = 0$ 消去约束即可，所以无论哪种情况都会得到：

$$\lambda g(x)=0$$

在等式约束优化中，约束函数与目标函数的梯度只要满足平行即可，而在不等式约束中则不然，若 $\lambda \neq 0$，则可行解 $x$ 是落在约束区域的边界上，这时可行解应尽量靠近无约束时的解，所以在约束边界上，目标函数的负梯度方向应该远离约束区域朝向无约束时的解，此时正好可得约束函数的梯度方向与目标函数的负梯度方向应相同：

$$-\nabla_xf(x)=\lambda\nabla_xg(x)$$

上式需要满足 $\lambda > 0$ ，这个问题可以举一个形象的例子，假设你去爬山，目标是山顶，但有一个障碍挡住了通向山顶的路，所以只能沿着障碍爬到尽可能靠近山顶的位置，然后望着山顶叹叹气，这里山顶便是目标函数的可行解，障碍便是约束函数的边界，此时的梯度方向一定是指向山顶的，与障碍的梯度同向，下图描述了这种情况:

![](https://darknessbeforedawn.github.io/test-book/images/SVM4.png)

对于不等式约束，只要满足一定的条件，依然可以使用拉格朗日乘子法解决，这里的条件便是 $KKT$ 条件。

对于以下约束问题:

$$\begin{aligned}
&\min_xf(x)  \\  
&s.t.~~~~h_i(x)=0,i = 1,2,...,m \\
&~~~~~~~~~~g(x) \leq 0 , j = 1,2,...,n \\ 
\end{aligned}$$

对应拉格朗日函数：

$$L(x,\alpha,\beta)=f(x)+\sum_{i=1}^m\alpha_ih_i(x)+\sum_{j=1}^n\beta_ig_i(x)$$

则不等式约束后的可行解 $x$ 需要满足的 $KKT$ 条件为：

$$\begin{align}
\nabla_x L(x,\alpha,\beta) &= 0   \\ 
\beta_jg_j(x) &= 0  , \ j=1,2,...,n\\ 
h_i(x)&= 0 , \ i=1,2,...,m  \\ 
g_j(x) &\le 0  , \  j=1,2,...,n  \\ 
\beta_j &\ge  0 , \ j=1,2,...,n  \\ 
\end{align} $$

满足 $KKT$ 条件后极小化拉格朗日函数即可得到在不等式约束条件下的可行解。 $KKT$ 条件:

(1) ：拉格朗日取得可行解的必要条件；

(2) ：这就是以上分析的一个比较有意思的约束，称作松弛互补条件；

(3)(4) ：初始的约束条件；

(5) ：不等式约束的 Lagrange Multiplier 需满足的条件。

主要的 $KKT$ 条件便是 (3) 和 (5) ，只要满足这俩个条件便可直接用拉格朗日乘子法， $SVM$ 中的支持向量便是来自于此，需要注意的是 KKT 条件与对偶问题也有很大的联系。

### 1.2 对偶问题

对于任意一个带约束的优化都可以写成如下形式：

$$\begin{aligned}  
&\min_x \  f(x)  \\  
&s.t.  \ \ \ h_i(x) = 0 , \  i = 1,2,...,m \ \\  
& \ \ \ \ \ \ \ \ \ \   g_j(x) \le 0, \  j = 1,2,...,n 
\end{aligned}$$

如果 $g_j(x)$ 全是凸函数，并且 $h_i(x)$ 全是仿射函数（ $Ax+b$ 的形式），上述优化就是一个凸优化问题，凸优化极值唯一。

定义如下 $Lagrangian$ ：

$$
L(x,\alpha,\beta) =f(x) + \sum_{i=1}^m \alpha_i h_i(x) + \sum_{j=1}^n\beta_jg_j(x)
$$

它通过一些系数把约束条件和目标函数结合起来，将带约束的优化问题转化为无约束问题。

现在我们针对参数 $\alpha,\beta$ 对 $L(x,\alpha,\beta)$ 取最大值，令:

$$
Z(x) =\max_{\alpha_i,\beta_j \geq 0}L(x,\alpha,\beta)
$$

满足约束条件的 $x$ 使得 $h_i(x)=0$ ,并且 $g_j(x) \leq 0,\beta_j \geq 0$ ,则有 $\beta_jg_j(x) \leq 0$ .因此对于满足约束条件的 $x$ 有 $f(x)=Z(x)$ .而对于那些不满足约束条件的 $x$ 有 $Z(x)=\infty$ ,这样将导致问题无解，因此必须满足约束条件。这样一来，原始带约束的优化问题等价于如下无约束优化问题：

$$
\min_x f(x) = \min_xZ(x) = \min_x \max_{\alpha_i,\beta_j \geq 0}L(x,\alpha,\beta)
$$

这个问题称作原问题(primal problem),与之相对应的为对偶问题(dual problem),器形式非常类似，只是把 $\min,\max$ 交换了一下：

$$
\max_{\alpha_i,\beta_j \geq 0}\min_xL(x,\alpha,\beta)
$$

原问题是在最大值中取最下的那个，对偶问题是在最小值取最大的那个。令：

$$
D(\alpha, \beta)= \min_xL(x,\alpha,\beta)
$$

如果原问题的最小值记为 $p^*$ ,那么对于所有 $\alpha_i,\beta_j \geq 0$ 有：

$$
D(\alpha, \beta) \leq p^*
$$

由于对于极值点(包括所有满足约束条件的点) $x^*$ , 并且 $\beta_j \geq 0$ ,总有 :

$$
\sum_{i=1}^m \alpha_i h_i(x^*) + \sum_{j=1}^n\beta_jg_j(x^*) \leq 0
$$ 

因此

$$
L(x^*,\alpha,\beta) = f(x^*) +\sum_{i=1}^m \alpha_i h_i(x^*) + \sum_{j=1}^n\beta_jg_j(x^*) \leq f(x^*)
$$

于是

$$
D(\alpha, \beta)= \min_xL(x,\alpha,\beta) \leq L(x^*,\alpha,\beta) \leq f(x^*) = p^*
$$

因此 

$$
\max_{\alpha_i,\beta_j \geq 0}D(\alpha, \beta)
$$

实际上就是原问题的最大下界，最大下界离我们要逼近的值最近。记对偶问题的最优值为 $d^*$ ，则有：

$$d^* \leq p^*$$

这个性质叫做弱对偶性（weak duality），对于所有优化问题都成立，即使原始问题非凸。这里还有两个概念： $f(x)–D(\alpha, \beta)$ 叫做对偶间隔（duality gap）， $p^*–d^*$ 叫做最优对偶间隔（optimal duality gap）。无论原始问题是什么形式，对偶问题总是一个凸优化的问题，这样对于那些难以求解的原始问题 （甚至是 NP 问题），均可以通过转化为偶问题，通过优化这个对偶问题来得到原始问题的一个下界， 与弱对偶性相对应的有一个强对偶性（strong duality） ，强对偶即满足：

$$d^* = p^*$$

强对偶是一个非常好的性质，因为在强对偶成立的情况下，可以通过求解对偶问题来得到原始问题的解，在 $SVM$ 中就是这样做的。当然并不是所有的对偶问题都满足强对偶性 ，在 $SVM$ 中是直接假定了强对偶性的成立，其实只要满足一些条件，强对偶性是成立的，比如说 $Slater$ 条件与 $KKT$ 条件。

#### $Slater$ 条件

若原始问题为凸优化问题，且存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $g_j(x)\leq 0$ 中的“ $≤$ ”严格取到“ $<$ ”，即存在 $x$ 满足 $g_j(x)<0 ,i=1,2,…,n$ ，则存在 $x^*,α^*,β^*$ 使得 $x^*$ 是原始问题的解， $α^*,β^*$ 是对偶问题的解，且满足：

$$p^* = d^* = L(x^*,\alpha^* ,\beta^*)$$

也就是说如果原始问题是凸优化问题并且满足 $Slater$ 条件的话，那么强对偶性成立。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。例如，对于某些非凸优化的问题，强对偶也成立。$SVM$ 中的原始问题 是一个凸优化问题（二次规划也属于凸优化问题），$Slater$ 条件在 $SVM$ 中指的是存在一个超平面可将数据分隔开，即数据是线性可分的。当数据不可分时，强对偶是不成立的，这个时候寻找分隔平面这个问题本身也就是没有意义了，所以对于不可分的情况预先加个 $kernel$ 就可以了。

#### $KKT$ 条件

假设 $x^*$ 与 $α^*,β^*$ 分别是原始问题（并不一定是凸的）和对偶问题的最优解，且满足强对偶性，则相应的极值的关系满足：

$$
\begin{aligned}  f(x^*) &= d^* = p^* =D(\alpha^*,\beta^*)  \\  &=\min_x f(x)+ \sum_{i = 1}^m \alpha_i^*h_i(x) + \sum_{j=1}^n\beta_j^*g_j(x) \\  & \le f(x^*)+ \sum_{i = 1}^m \alpha_i^*h_i(x^*) + \sum_{j=1}^n\beta_j^*g_j(x^*) \\ &\le f(x^*)  \end{aligned}
$$

由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^*$ 是 $L(x,\alpha^*,\beta^*)$ 的一个极值点，因此 $L(x,\alpha^*,\beta^*)$ 在 $x^*$ 处的梯度为0，即: 

$$\nabla_{x^*} L(x,\alpha^*,\beta^*) = 0$$

由第二个不等式，并且 $\beta_j^*g_j(x^*)$ 都是非正的，因此有：

$$\beta_j^*g_j(x^*)=0, j=1,\cdots,m$$

显然，如果 $\beta_j^* > 0$ ,那么必定有 $g_j(x^*)=0$ ,如果 $g_j(x^*) <0$ ,那么可以得到 $\beta_j^* = 0$ 。再将其他一些显而易见的条件写到一起，就是传说中的 KKT (Karush-Kuhn-Tucker) 条件：

$$
\begin{align}
\nabla_x L(x,\alpha,\beta) &= 0   \\ 
\beta_jg_j(x) &= 0  , \ j=1,2,...,n\\ 
h_i(x)&= 0 , \ i=1,2,...,m  \\ 
g_j(x) &\le 0  , \  j=1,2,...,n  \\ 
\beta_j &\ge  0 , \ j=1,2,...,n  \\ 
\end{align}
$$

总结来说就是说任何满足强对偶性的优化问题，只要其目标函数与约束函数可微，任一对原始问题与对偶问题的解都是满足 $KKT$ 条件的。即满足强对偶性的优化问题中，若 $x^*$ 为原始问题的最优解，$α^*,β^*$ 为对偶问题的最优解，则可得 $x^*,α^*,β^*$ 满足 $KKT$ 条件。

上面只是说明了必要性，当满足原始问题为凸优化问题时，必要性也是满足的，也就是说当原始问题是凸优化问题,且存在 $x^*,α^*,β^*$ 满足 $KKT$ 条件，那么它们分别是原始问题和对偶问题的极值点并且强对偶性成立.

##### 证明
首先原始问题是凸优化问题，固定 $α^*,β^*$ 之后对偶问题 $D(α^*,β^*)$ 也是一个凸优化问题，$x^*$ 是 $L(x,α^*,β^*)$ 的极值点：

$$
\begin{aligned}  
D(\alpha^*,\beta^*)  &= \min_x L(x,\alpha^*,\beta^*) \\ 
&= L(x^*,\alpha^*,\beta^*) \\ 
& =f(x^*)+\sum_{i=1}^m\alpha_i^*h_i(x^*)+\sum_{j=1}^n\beta_j^*g_j(x^*) \\  
&= f(x^*)  
\end{aligned}
$$

最后一个式子是根据 $KKT$ 条件中的 $h_i(x)=0$ 与 $\beta_jg_j(x)=0$ 得到的。这样一来，就证明了对偶间隔为零，也就是说，强对偶成立。


对于一个约束优化问题，找到其对偶问题，当弱对偶成立时，可以得到原始问题的一个下界。而如果强对偶成立，则可以直接求解对偶问题来解决原始问题。 $SVM$ 就是这样的。对偶问题由于性质良好一般比原始问题更容易求解，在 $SVM$ 中通过引入对偶问题可以将问题表示成数据的内积形式从而使得 kernel trick 的应用更加自然。此外，还有一些情况会同时求解对偶问题与原始问题 ，比如在迭代求解的过程中，通过判断对偶间隔的大小，可以得出一个有效的迭代停止条件。


### 1.3 空间

#### 向量空间

向量空间一个最大的特征是对加法运算和数乘运算封闭。 $n$ 维向量空间的定义是n维实向量全体构成的集合，同事考虑到向量的线性运算，成为实n维向量空间，用 $R^n$ 表示，显然 $Rn$ 中任意两个向量的和向量还是 $R^n$ 中的向量，$R^n$ 中任意一个向量与一个实数的乘积也是 $R^n$ 中的向量。向量空间又称为线性空间。

#### 欧氏空间

欧氏空间也称为欧几里得空间，是带有“内积”的实数域上的一类向量空间。引入内积的目的是能够计算两点间的距离和夹角。向量空间中的向量对应于欧几里得平面中的点，在向量空间中的加法运算对应于欧几里得空间中的平移。


#### [希尔伯特空间](http://www.cnblogs.com/ben-ben/articles/3391781.html)

希尔伯特空间即是完备的内积空间(欧氏空间)，首先说明一下完备性。完备空间或者完备度量空间是指空间中的任何柯西序列都收敛在该空间之内。柯西序列中的元素随着序数的增加而愈发靠近。更确切的说，在去掉优先个元素后，可以使得余下的元素中的任何两点间的距离的最大值不超过任意给定的正常数。