---
title: 贝叶斯分类器
layout: post
share: false
---
# 1.贝叶斯公式

#### 条件概率公式

设 $A$ , $B$ 是两个事件，且 $P(B)>0$ ,则在事件 $B$ 发生的条件下，事件 $A$ 发生的条件概率(conditional probability)为：

$$P(A|B) = \frac{P(AB)}{P(B)}$$

#### 全概率公式

如果事件 $B_1,B_2,\cdots,B_n$ 满足 $B_i\cap B_j = \emptyset,i\neq j~~~~~i,j=1,2,\cdots,n$ , 且 $P(B_i)>0,B_1\cup B_2\cup\cdots B_n=\Omega$ ，设 $A$ 为任意事件，则有:

$$P(A)=\sum_{i=1}^nP(B_i)P(A|B_i)$$

上式即为全概率公式(formula of total probability)

全概率公式的意义在于，当直接计算 $P(A)$ 较为困难,而 $P(B_i),P(A$ \| $Bi)$ 的计算较为简单时，可以利用全概率公式计算 $P(A)$。思想就是，将事件 $A$ 分解成几个小事件，通过求小事件的概率，然后相加从而求得事件A的概率，而将事件A进行分割的时候，不是直接对 $A$ 进行分割，而是先找到样本空间 $\Omega$ 的一个个划分 $B_1,B_2,\cdots,B_n$ ,这样事件 $A$ 就被事件 $AB_1,AB_2,\cdots,AB_n$ 分解成了 $n$ 部分，而每一个 $B_i$ 发生都可能导致 $A$ 发生相应的概率是 

$$P(A|B_i)$$

#### 贝叶斯公式

与全概率公式解决的问题相反，贝叶斯公式是建立在条件概率的基础上寻找事件发生的原因（事件 $A$ 已经发生的条件下，分割中的小事件 $B_i$ 的概率），设 $B_1,B_2,\cdots,B_n$ 是样本空间 $\Omega$ 的一个划分，则对任一事件 $A(P(A)>0)$ ,有

$$P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum\limits_{j=1}^nP(B_j)P(A|B_j)}$$

上式即为贝叶斯公式(Bayes formula)， $B_i$ 常被视为导致试验结果 $A$ 发生的\原因， $P(B_i)$ 表示各种原因发生的可能性大小，故称先验概率； $P(Bi$ \| $A)$ 则反映当试验产生了结果 $A$ 之后，再对各种原因概率的新认识，故称后验概率。

# 2.朴素贝叶斯

设输入空间 $\mathcal{X}\subseteq \mathbf{R^n}$ 为 $n$ 维向量的集合，输出空间为类标记集合$$\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$$.输入为特征向量 $x\in\mathcal{X}$ ,输出为类标记(class label) $y\in\mathcal{Y}$ . $X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量， $Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。 $P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

由 $P(X,Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$ .具体地，学习以下先验概率分布及条件概率分布。先验概率分布

$$P(Y=c_k),~~~k=1,2,\cdots,K$$

条件概率分布

$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),~~~k=1,2,\cdots,K$$

于是学习到联合概率分布 $P(X,Y)$ .

条件概率分布 $P(X=x|Y=c_k)$ 有指数级数量的参数，其估计实际不可行的。事实上，假设 $x^{(j)}$ 可取值有 $S_j$ 个， $j=1,2,\cdots,n,~~Y$ 可取值有 $K$ 个，那么参数个数为 $K\prod\limits_{j=1}nS_j$ .
