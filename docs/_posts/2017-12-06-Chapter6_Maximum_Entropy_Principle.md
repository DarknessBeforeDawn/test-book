---
title: 最大熵模型
layout: post
share: false
---

# 1.熵
信息是个很抽象的概念，接下来我们从[文件压缩](http://www.ruanyifeng.com/blog/2014/09/information-entropy.htm)问题来说明信息熵，以便于理解。

#### 压缩原理
压缩的本质就是将重复出现的字符用更短的符号代替，就是找出文件内容的概率分布，将出现概率高德部分替代成更短的形式。因此，内容越是重复的文件就可以压缩的越小。比如，"ABABABABABABAB"可以压缩为"7AB"。而内容毫无重复，就很难进行压缩，例如无理数$\pi$就很难压缩。

#### 压缩极限

压缩可以分解成两个步骤。第一步是得到文件内容的概率分布；第二步是对文件进行编码，用较短的符号替代那些重复出现的部分。

比如扔硬币的结果，那么只要一个二进制位就够了，1表示正面，0表示表示负面。足球赛的结果，最少需要两个二进制位。扔骰子的结果，最少需要三个二进制位。

在均匀分布的情况下，假定一个字符（或字符串）在文件中出现的概率是$p$，那么该文件种最多可能出现$\frac{1}{p}$种字符（字符串）。则需要$\log_2\frac{1}{p}$个二进制位才能表示$\frac{1}{p}$种情况。也就是每种字符（或字符串）需要占用$\log_2\frac{1}{p}$个二进制位。那么全部压缩该文件总共需要$\frac{1}{p}\log_2\frac{1}{p}$个二进制位。

更一般情况，假定文件有n个部分组成，并且每个部分的内容在文件中的出现概率分别为$p_1,p_2,\cdots p_n$。可推出如下公式：

$$
\log_2\frac{1}{p_1}+\log_2\frac{1}{p_2}+\cdots+\log_2\frac{1}{p_n}
=\sum_{i=1}^n\log_2\frac{1}{p_i}
$$

上述公式即为压缩极限，表示压缩所需要的二进制位数。

#### 信息熵
在均匀分布的情况下$(p_i=\frac{1}{n})$，压缩每个字符（字符串）平均需要$\sum\limits_{i=1}^n\frac{1}{n}\log_2\frac{1}{p_i}$个二进制位。对于一般情况下$p_i$不等，压缩每个字符（字符串）平均需要$\sum\limits_{i=1}^np_i\log_2\frac{1}{p_i}$个二进制位。$\sum\limits_{i=1}^np_i\log_2\frac{1}{p_i}$即为信息熵公式。

假定有两个文件都包含1024个符号，在ASCII码的情况下，它们的长度是相等的，都是1KB。甲文件的内容50%是a，30%b，20%是c，则平均每个符号要占用1.49个二进制位。

$$0.5*\log_2\frac{1}{0.5}+0.3*log_2\frac{1}{0.3} + 0.2*log_2\frac{1}{0.2}=1.47$$

乙文件的内容10%是a，10%是b,$\cdots$,10%是j，则平均每个符号要占用3.32个二进制位。

$$0.1*\log_2\frac{1}{0.1}*10=3.32$$

可以看到文件内容越是分散（随机），所需要的二进制位就越长。所以，这个值可以用来衡量文件内容的随机性（又称不确定性）。这就叫做信息熵（information entropy）。

注:

（1）信息熵只反映内容的随机性，与内容本身无关。

（2）信息熵越大，表示占用的二进制位越长，因此就可以表达更多的符号(信息量并不一定大也许是一堆无序没意义的字符)。

（3）信息熵与热力学的熵，基本无关。

# 2.最大熵模型





  

